{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ac7b6ca5-ce3d-43c7-95ee-c6f21bab3d24",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Data transformation in Databricks\n",
    "- PySpark to perform the same operations on DataFrames in a distributed fashion, which is more suitable for larger datasets\n",
    "- PySpark operations are similar to the Pandas operations but are designed to work with distributed data in a Spark cluster, making them suitable for handling large datasets.\n",
    "- But depends upon what kind of transformation you are doing (you don't always need pyspark)\n",
    "- First pyspark example\n",
    "- Do the same with SQL\n",
    "- Finally with pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "94d7cbf0-70d9-43e3-97d0-e0e8d8f22d46",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Create a sample DataFrame 'df1' with 100,000 rows\n",
    "data1 = [(i, f'Name_{i}', random.randint(60, 100)) for i in range(1, 100001)]\n",
    "df1 = spark.createDataFrame(data1, [\"ID\", \"Name\", \"Score1\"])\n",
    "\n",
    "# Create another sample DataFrame 'df2' with 100,000 rows\n",
    "data2 = [(random.randint(1, 100000), random.randint(60, 100), f'City_{i}') for i in range(1, 100001)]\n",
    "df2 = spark.createDataFrame(data2, [\"ID\", \"Score2\", \"City\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "93b56fe5-9b73-4613-9c98-d064f774a9a4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b1823b08-3c1b-4c8e-8e6f-34af8b9eb287",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import min, max\n",
    "\n",
    "df1.agg(min(\"ID\")).collect()[0][0], df1.agg(max(\"ID\")).collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5a3eb4a1-0d8d-4066-aad6-4fcad2e840a0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f9d43a76-dde8-465f-88f7-8f074e666fc6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import min, max\n",
    "\n",
    "df2.agg(min(\"ID\")).collect()[0][0], df2.agg(max(\"ID\")).collect()[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3fad2c3c-8eb3-4713-9d5b-f44db9f2ce8f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Create a calculated field\n",
    "Data transformation can involve various operations like filtering, sorting, aggregating, or modifying columns. \n",
    "\n",
    "Here's an example of adding a new column to df1 by multiplying Score1 by 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bfde2f06-57d7-4b68-8048-e3f305b1e8eb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df1 = df1.withColumn(\"Score1_double\", col(\"Score1\") * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9d2c4864-6dda-49c9-92f8-84b0193b49ba",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7288ace4-fad9-4abc-97b3-7dd0224a33a0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Inner Join\n",
    "- To perform an inner join using PySpark, you can use the join method. Here's how to do an inner join between `df1` and `df2` on the `ID` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f4ddd37a-30fa-4b1c-8eb7-6e7fa6a86945",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "inner_join_df = df1.join(df2, on=\"ID\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "78aa9572-eaaa-4c2c-b085-fdfb3f7106da",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(inner_join_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3df37955-8d4f-4b46-8178-c467fd383cb0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Left Join \n",
    "To perform a left join, you can use the join method with the how parameter set to \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4691fdd5-d5de-403b-b489-a257d2961863",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "left_join_df = df1.join(df2, on=\"ID\", how=\"left\")\n",
    "display(left_join_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "624381ef-1bb6-4903-b29d-513693b29794",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Right Join\n",
    "To perform a right join, you can use the join method with the how parameter set to \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "00e201a4-aad7-475c-8f0e-eedd1e2ef128",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "right_join_df = df1.join(df2, on=\"ID\", how=\"right\")\n",
    "display(right_join_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "63f92c22-0ab8-4d65-a292-9fbf1934b46a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Now what if we want to use SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "30296ad1-0cef-430f-8cb1-061904fc1e9d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Create a sample DataFrame 'df1' with 100,000 rows\n",
    "data1 = [(i, f'Name_{i}', random.randint(60, 100)) for i in range(1, 100001)]\n",
    "df1 = spark.createDataFrame(data1, [\"ID\", \"Name\", \"Score1\"])\n",
    "\n",
    "# Create another sample DataFrame 'df2' with 100,000 rows\n",
    "data2 = [(random.randint(1, 100000), random.randint(60, 100), f'City_{i}') for i in range(1, 100001)]\n",
    "df2 = spark.createDataFrame(data2, [\"ID\", \"Score2\", \"City\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "964e5bdb-4eb7-4e72-9b9e-a9d52b4a9b89",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Register 'df1' as a SQL table\n",
    "df1.createOrReplaceTempView(\"table1\")\n",
    "\n",
    "# Register 'df2' as a SQL table\n",
    "df2.createOrReplaceTempView(\"table2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ed6e95fa-259d-46b2-a525-79734b1d601e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Create a new calculated field 'Score1_double' by multiplying 'Score1' by 2\n",
    "SELECT *, Score1 * 2 AS Score1_double FROM table1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a3f2d430-afcd-4405-96ee-be763b1ac7db",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Inner join 'table1' and 'table2' on 'ID' column\n",
    "SELECT * FROM table1\n",
    "INNER JOIN table2 ON table1.ID = table2.ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b8e65b32-4715-485c-bdb3-d53aaf40b28c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Left join 'table1' and 'table2' on 'ID' column\n",
    "SELECT * FROM table1\n",
    "LEFT JOIN table2 ON table1.ID = table2.ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "efd62b95-94ff-4ca6-86bb-be0480b1f53b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Right join 'table1' and 'table2' on 'ID' column\n",
    "SELECT * FROM table1\n",
    "RIGHT JOIN table2 ON table1.ID = table2.ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "47475515-60a8-4739-9fea-df16f7e1d8f6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from table2;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f7329b6e-0e98-46e1-bdc5-e3219a51dc77",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Group by the \"City\" column and calculate the average \"Score2\" for each city\n",
    "SELECT City, AVG(Score2) AS Average_Score2\n",
    "FROM table2\n",
    "GROUP BY City;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6164b616-accc-4cff-b451-48ab80aa7866",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql \n",
    "-- Create a new column 'Total_Score' by adding 'Score1' and 'Score2'\n",
    "SELECT *, Score1 + Score2 AS Total_Score FROM (\n",
    "    SELECT * FROM table1\n",
    "    INNER JOIN table2 ON table1.ID = table2.ID\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e2c315ab-54d9-404b-8b9b-557463af4cef",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(_sqldf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bb5a19eb-5650-4bc1-8035-31645cc513a4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pddf = _sqldf.toPandas()\n",
    "display(pddf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2e1d0757-883b-4aa1-a37e-7e566bd2db56",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pddf.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3cd54b3b-6eb3-46eb-9903-e6b3c2f462d9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Using pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "182997f7-eb88-4d25-add3-2e31a95866e1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Create a sample DataFrame 'df1' with 100,000 rows\n",
    "data1 = {\n",
    "    'ID': range(1, 100001),\n",
    "    'Name': [f'Name_{i}' for i in range(1, 100001)],\n",
    "    'Score1': [random.randint(60, 100) for _ in range(100000)],\n",
    "}\n",
    "\n",
    "df1 = pd.DataFrame(data1)\n",
    "\n",
    "# Create another sample DataFrame 'df2' with 100,000 rows\n",
    "data2 = {\n",
    "    'ID': [random.randint(1, 100000) for _ in range(100000)],\n",
    "    'Score2': [random.randint(60, 100) for _ in range(100000)],\n",
    "    'City': [f'City_{i}' for i in range(1, 100001)],\n",
    "}\n",
    "\n",
    "df2 = pd.DataFrame(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0cc7de3a-ea22-48ea-a532-6d505eed7367",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "834fff72-46f4-47de-8a24-80c4d68c9d3b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "11a167a8-d510-4043-9afb-db640e99e527",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "inner_join_df = pd.merge(df1, df2, on='ID', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "da1fd43a-b048-45d3-bcf4-26bd7c017dfe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(inner_join_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5c657b75-940c-4e3b-a96e-3838743a0694",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1['Score1_double'] = df1['Score1'] * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e306f44-e8a2-461b-b8cb-5e1647507827",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "inner_join_df = pd.merge(df1, df2, on='ID', how='inner')\n",
    "display(inner_join_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9531ac20-9984-42f3-b591-67022e6fcd98",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "left_join_df = pd.merge(df1, df2, on='ID', how='left')\n",
    "display(left_join_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "60d274e2-31ab-48d4-b113-33c3421144fd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "right_join_df = pd.merge(df1, df2, on='ID', how='right')\n",
    "display(right_join_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aa87fb58-22a4-4705-8192-222e74a9618b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "inner_join_df['Total_Score'] = inner_join_df['Score1'] + inner_join_df['Score2']\n",
    "display(inner_join_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2f4da1de-29cb-459b-90b0-c80b9f47f9bb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "data-transformation",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
